# üìä Gu√≠a: C√≥mo Verificar el Uso de GPU en Alfred

Esta gu√≠a te muestra **todas las formas** de verificar si Alfred est√° usando tu GPU correctamente.

## ‚úÖ Estado Actual

**GPU Detectada:** NVIDIA GeForce RTX 4060 (8.59 GB)  
**Estado:** ‚úì Funcionando correctamente  
**CUDA Version:** 11.8

---

## üîç M√©todos de Verificaci√≥n

### **M√©todo 1: Demo de GPU (M√°s R√°pido)**

Este script realiza una verificaci√≥n completa y r√°pida:

```powershell
python demo_gpu.py
```

**Qu√© muestra:**
- ‚úì Detecci√≥n de hardware
- ‚úì Informaci√≥n de la GPU
- ‚úì Configuraci√≥n de Ollama
- ‚úì Prueba de operaci√≥n en GPU
- ‚úì Uso de memoria

**Resultado esperado:** Deber√≠as ver "GPU detectada: S√ç" y operaciones exitosas en CUDA.

---

### **M√©todo 2: Monitor en Tiempo Real (Python)**

Para ver el uso de GPU mientras Alfred est√° corriendo:

```powershell
# En una terminal separada
python monitor_gpu_usage.py
```

**Caracter√≠sticas:**
- Actualizaci√≥n cada 2 segundos
- Muestra memoria asignada y reservada
- Porcentaje de uso
- Presiona Ctrl+C para detener

**Uso personalizado:**
```powershell
# Actualizar cada 5 segundos
python monitor_gpu_usage.py 5
```

---

### **M√©todo 3: NVIDIA System Management Interface (nvidia-smi)**

La herramienta oficial de NVIDIA para monitorear GPU:

#### **Comando √önico:**
```powershell
nvidia-smi
```

**Muestra:**
- Uso de GPU (%)
- Memoria usada vs total
- Temperatura
- Procesos activos usando GPU
- Consumo de energ√≠a

#### **Monitor Continuo:**
```powershell
# Actualizar cada 2 segundos
nvidia-smi -l 2
```

#### **Monitor Autom√°tico (Script PowerShell):**
```powershell
.\monitor_gpu.ps1
```

Este script formatea mejor la informaci√≥n y actualiza autom√°ticamente.

---

### **M√©todo 4: Verificar Procesos Usando GPU**

Para ver qu√© procesos espec√≠ficamente est√°n usando la GPU:

```powershell
nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv
```

**Buscar procesos de Alfred/Ollama:**
```powershell
nvidia-smi | Select-String -Pattern "ollama|python|alfred"
```

---

### **M√©todo 5: Administrador de Tareas de Windows**

1. Abre el **Administrador de Tareas** (Ctrl+Shift+Esc)
2. Ve a la pesta√±a **Rendimiento**
3. Selecciona **GPU** en el panel izquierdo

**Qu√© ver:**
- **GPU 0 - 3D:** Procesamiento general
- **GPU 0 - Compute:** Computaci√≥n (esto es lo que usa Alfred)
- **Memoria GPU dedicada:** Cu√°nta memoria est√° siendo utilizada

Cuando Alfred/Ollama est√©n procesando, ver√°s:
- üìà Aumento en "Compute"
- üíæ Aumento en "Memoria GPU dedicada"

---

### **M√©todo 6: Dentro de Alfred (Program√°tico)**

Si quieres verificar desde c√≥digo Python:

```python
from gpu_manager import get_gpu_manager

# Obtener gestor
gpu = get_gpu_manager()

# Verificar si est√° usando GPU
print(f"GPU disponible: {gpu.has_gpu}")
print(f"Dispositivo: {gpu.device}")
print(f"Tipo: {gpu.device_type}")

# Ver uso de memoria en tiempo real
if gpu.has_gpu:
    mem = gpu.get_memory_usage()
    print(f"Memoria usada: {mem['allocated']:.2f} GB")
    print(f"Memoria reservada: {mem['reserved']:.2f} GB")
```

---

## üéØ Se√±ales de que la GPU S√ç se est√° Usando

### **En Ollama:**
- ‚úì Al iniciar Ollama, ver√°s: `"GPU available: true"`
- ‚úì Los modelos se cargan en GPU: `"loaded model into GPU"`
- ‚úì nvidia-smi muestra proceso "ollama" activo

### **En Alfred:**
- ‚úì `demo_gpu.py` muestra "GPU detectada: S√ç"
- ‚úì PyTorch device es "cuda", no "cpu"
- ‚úì Las operaciones se ejecutan m√°s r√°pido
- ‚úì nvidia-smi muestra uso de memoria GPU cuando procesas consultas

### **Diferencia de Rendimiento:**
| Operaci√≥n | Con GPU (RTX 4060) | Sin GPU (CPU) |
|-----------|-------------------|---------------|
| Cargar modelo | 2-5 seg | 10-30 seg |
| Consulta simple | 0.5-2 seg | 5-15 seg |
| Documentos grandes | 3-8 seg | 20-60 seg |

---

## üîß Troubleshooting

### **Si no detecta la GPU:**

1. **Verificar drivers NVIDIA:**
   ```powershell
   nvidia-smi
   ```
   Si no funciona, actualiza drivers desde: https://www.nvidia.com/Download/index.aspx

2. **Verificar PyTorch con CUDA:**
   ```powershell
   python -c "import torch; print(f'CUDA disponible: {torch.cuda.is_available()}')"
   ```

3. **Reinstalar PyTorch con CUDA:**
   ```powershell
   pip uninstall torch torchvision torchaudio
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   ```

4. **Verificar variables de entorno de Ollama:**
   ```powershell
   $env:OLLAMA_GPU
   ```
   Deber√≠a ser "1"

---

## üìà Comandos √ötiles R√°pidos

```powershell
# Verificaci√≥n r√°pida
python demo_gpu.py

# Monitor mientras usas Alfred
python monitor_gpu_usage.py

# Ver uso actual
nvidia-smi

# Ver procesos espec√≠ficos
nvidia-smi pmon

# Ver solo memoria
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# Temperatura
nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader

# Monitor continuo cada 1 segundo
nvidia-smi -l 1
```

---

## üí° Recomendaciones para Tu RTX 4060

Con 8.59 GB de VRAM, puedes usar c√≥modamente:

### **Modelos Recomendados:**
- ‚úì **qwen2.5:7b** (actual) - √ìptimo
- ‚úì **gemma2:9b** - Excelente rendimiento
- ‚úì **llama3:8b** - Muy bueno
- ‚úì **mixtral:8x7b** (cuantizado) - Si quieres m√°s poder

### **Configuraciones √ìptimas:**
```python
# En config.py
OLLAMA_MODEL = "qwen2.5:7b"  # o gemma2:9b
CHUNK_SIZE = 1000
K_RETRIEVER = 5  # Puedes aumentar a 7-10 con tu GPU
```

### **Para Aprovechar al M√°ximo:**
- Procesar documentos m√°s grandes
- Usar m√°s contexto (k=7 o k=10)
- Modelos de embedding m√°s potentes

---

## üé¨ Flujo de Trabajo Recomendado

1. **Antes de iniciar Alfred:**
   ```powershell
   python demo_gpu.py
   ```
   ‚Üí Verifica que GPU est√° disponible

2. **Iniciar monitor (terminal 1):**
   ```powershell
   python monitor_gpu_usage.py
   ```

3. **Iniciar Alfred (terminal 2):**
   ```powershell
   python alfred.py
   ```

4. **Observar el uso de GPU en el monitor** mientras haces consultas

---

## üìû Soporte

Si la GPU no se detecta:
1. Ejecuta: `python demo_gpu.py` y guarda la salida
2. Ejecuta: `nvidia-smi` y guarda la salida
3. Verifica que Ollama est√© configurado: `ollama show qwen2.5:7b`

**Tu configuraci√≥n actual est√° funcionando correctamente** ‚úì
