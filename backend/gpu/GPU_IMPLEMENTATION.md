# Implementaci√≥n de Soporte GPU en Alfred

## üìå Resumen de Cambios

Se ha implementado un sistema completo de detecci√≥n y uso autom√°tico de GPU para acelerar los modelos de IA en Alfred.

## üÜï Archivos Nuevos

### 1. `gpu_manager.py`
**Gestor principal de GPU** que:
- ‚úÖ Detecta autom√°ticamente GPU NVIDIA (CUDA)
- ‚úÖ Detecta GPU AMD (ROCm)
- ‚úÖ Detecta Apple Silicon (Metal/MPS)
- ‚úÖ Configura Ollama para usar GPU
- ‚úÖ Aplica optimizaciones de inferencia
- ‚úÖ Gestiona memoria de GPU
- ‚úÖ Provee fallback autom√°tico a CPU

**Caracter√≠sticas principales:**
```python
from gpu_manager import get_gpu_manager

gpu = get_gpu_manager()

# Verificar si hay GPU
if gpu.has_gpu:
    print(f"GPU: {gpu.device_type}")
    
# Configurar Ollama
gpu.configure_ollama_for_gpu()

# Ver estado
print(gpu.get_status_report())

# Limpiar memoria
gpu.clear_cache()
```

### 2. `test_gpu.py`
Script completo de pruebas que:
- Detecta hardware disponible
- Configura Ollama
- Prueba operaciones en GPU
- Muestra uso de memoria
- Valida funcionamiento

**Uso:**
```powershell
python test_gpu.py
```

### 3. `demo_gpu.py`
Demostraci√≥n interactiva y amigable que:
- Muestra detecci√≥n paso a paso
- Da recomendaciones seg√∫n hardware
- Gu√≠a al usuario en configuraci√≥n

**Uso:**
```powershell
python demo_gpu.py
```

### 4. `GPU_SETUP.md`
Documentaci√≥n completa con:
- Instrucciones de instalaci√≥n
- Configuraci√≥n por plataforma (Windows/Linux/Mac)
- Soluci√≥n de problemas
- Optimizaciones de rendimiento
- Ejemplos de uso

## üîß Archivos Modificados

### 1. `alfred_core.py`
**Cambios:**
- ‚úÖ Importa `gpu_manager`
- ‚úÖ Inicializa GPU al crear instancia
- ‚úÖ Configura Ollama para GPU autom√°ticamente
- ‚úÖ Agrega m√©todos para consultar estado de GPU:
  - `get_gpu_status()` - Reporte completo
  - `get_gpu_memory_usage()` - Uso de memoria
  - `clear_gpu_cache()` - Limpiar cach√©
- ‚úÖ Incluye info de GPU en `get_info()`

**Ejemplo de uso:**
```python
from alfred_core import AlfredCore

alfred = AlfredCore()  # GPU se detecta y configura autom√°ticamente

# Ver estado
print(alfred.get_gpu_status())

# Ver memoria
memory = alfred.get_gpu_memory_usage()
if memory:
    print(f"Memoria: {memory['allocated']:.2f} GB")

# Limpiar si es necesario
alfred.clear_gpu_cache()
```

### 2. `alfred.py`
**Cambios:**
- ‚úÖ Importa y inicializa `gpu_manager` al inicio
- ‚úÖ Configura GPU antes de cargar modelos
- ‚úÖ Agrega comando `gpu` para ver estado interactivo
- ‚úÖ Permite limpiar cach√© desde CLI

**Nuevo comando:**
```
Tu pregunta: gpu

[Muestra estado de GPU y uso de memoria]
¬øLimpiar cach√© de GPU? (s/n):
```

### 3. `README.md`
**Cambios:**
- ‚úÖ Menciona soporte GPU en caracter√≠sticas
- ‚úÖ Agrega paso de verificaci√≥n GPU en instalaci√≥n
- ‚úÖ Agrega comando `gpu` en lista de comandos especiales
- ‚úÖ Link a `GPU_SETUP.md`

## üöÄ Caracter√≠sticas Implementadas

### Detecci√≥n Autom√°tica
```
‚úì GPU NVIDIA detectada: NVIDIA GeForce RTX 3060
  - Memoria: 12.00 GB
  - CUDA Version: 12.1
‚úì Ollama configurado para usar GPU NVIDIA CUDA
‚úì Optimizaciones CUDA aplicadas
```

### Fallback Transparente
Si no hay GPU:
```
‚Ñπ No se detect√≥ GPU dedicada, usando CPU
‚Ñπ Ollama configurado para usar CPU
```

### Gesti√≥n de Memoria
```python
# Ver uso actual
memory = gpu.get_memory_usage()
# {'allocated': 0.25, 'reserved': 0.5, 'max_allocated': 1.2}

# Limpiar cach√©
gpu.clear_cache()
```

### Variables de Entorno
Configuraci√≥n autom√°tica de Ollama:
- `OLLAMA_GPU=1` (si hay GPU)
- `OLLAMA_NUM_GPU=1` (n√∫mero de GPUs)
- `OLLAMA_GPU=0` (si solo CPU)

## üìä Rendimiento Esperado

### Con GPU NVIDIA (ej. RTX 3060)
- üöÄ **3-5x m√°s r√°pido** que CPU
- ‚úÖ Modelos grandes (gemma2:9b) sin problemas
- ‚úÖ Embeddings significativamente m√°s r√°pidos
- ‚úÖ Menor latencia en respuestas

### Con CPU (fallback)
- ‚öôÔ∏è Velocidad normal
- üìù Recomendado: modelos peque√±os (gemma2:2b)
- ‚è±Ô∏è Mayor latencia en respuestas

## üß™ C√≥mo Probar

### 1. Verificaci√≥n R√°pida
```powershell
python demo_gpu.py
```

### 2. Pruebas Completas
```powershell
python test_gpu.py
```

### 3. Usar con Alfred
```powershell
python alfred.py
```

Al iniciar ver√°s:
```
‚úì GPU NVIDIA detectada: [Tu GPU]
‚úì Ollama configurado para usar GPU
‚úì Optimizaciones aplicadas
```

### 4. Verificar Estado Durante Uso
```
Tu pregunta: gpu
```

## üîç Soluci√≥n de Problemas

### GPU no detectada

**1. Verificar driver (Windows):**
```powershell
nvidia-smi
```

**2. Verificar PyTorch:**
```python
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

**3. Reinstalar PyTorch con CUDA:**
```powershell
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

### Ollama no usa GPU

**1. Verificar variables:**
```python
python -c "import os; print(os.environ.get('OLLAMA_GPU'))"
```

**2. Reiniciar Ollama:**
```powershell
taskkill /F /IM ollama.exe
python alfred.py  # Inicia autom√°ticamente
```

### Errores de memoria

**1. Usar modelo m√°s peque√±o:**
```bash
# En .env
ALFRED_MODEL=gemma2:2b
```

**2. Limpiar cach√©:**
```
Tu pregunta: gpu
¬øLimpiar cach√© de GPU? s
```

## üìñ Documentaci√≥n Adicional

- **`GPU_SETUP.md`** - Gu√≠a completa de configuraci√≥n
- **`README.md`** - Documentaci√≥n principal actualizada
- **C√≥digo inline** - Todos los m√≥dulos tienen docstrings detallados

## ‚úÖ Compatibilidad

### Plataformas Soportadas
- ‚úÖ **Windows** con GPU NVIDIA
- ‚úÖ **Linux** con GPU NVIDIA
- ‚úÖ **Linux** con GPU AMD (ROCm)
- ‚úÖ **macOS** con Apple Silicon (M1/M2/M3)
- ‚úÖ **Cualquier plataforma** con CPU (fallback)

### Modelos Soportados
- ‚úÖ Ollama LLM (gemma2, llama3, etc.)
- ‚úÖ Ollama Embeddings (nomic-embed-text, etc.)
- ‚úÖ Cualquier modelo que use PyTorch

## üéØ Pr√≥ximos Pasos

Para el usuario:

1. **Probar detecci√≥n:**
   ```powershell
   python demo_gpu.py
   ```

2. **Verificar funcionamiento:**
   ```powershell
   python test_gpu.py
   ```

3. **Usar Alfred normalmente:**
   ```powershell
   python alfred.py
   ```
   
4. **Si tienes problemas:**
   - Revisa `GPU_SETUP.md`
   - Ejecuta `nvidia-smi` (NVIDIA)
   - Verifica instalaci√≥n de PyTorch

## üìù Notas Importantes

1. **No requiere configuraci√≥n manual** - Todo es autom√°tico
2. **Fallback seguro a CPU** - Siempre funcionar√°
3. **Sin cambios en API** - Compatibilidad total con c√≥digo existente
4. **Optimizaciones transparentes** - El usuario no necesita hacer nada

## üí° Ejemplo Completo

```python
# Alfred detecta y usa GPU autom√°ticamente
from alfred_core import AlfredCore

# Iniciar (GPU se configura aqu√≠)
alfred = AlfredCore()
# ‚úì GPU NVIDIA detectada...
# ‚úì Ollama configurado para usar GPU...

# Usar normalmente
response = alfred.ask("¬øCu√°l es mi RFC?")
print(response['answer'])

# Ver estado de GPU
print(alfred.get_gpu_status())

# Limpiar memoria si es necesario
alfred.clear_gpu_cache()
```

---

**Implementado con √©xito** ‚úÖ

Todo est√° listo para usar. El sistema detectar√° autom√°ticamente la GPU y la usar√° si est√° disponible, o usar√° CPU de forma transparente.
