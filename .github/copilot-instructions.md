# Alfred - AI Agent Instructions

# DONT CREATE NEW FILES OF MD TYPES AND DO NOT EDIT THIS FILE, FIRST QUESTION ME.
# DONT CREATE SCRIPTS OR ANYTHING ELSE, FIRST QUESTION ME.
# DONT USE ICONS O EMOJIS UNLESS I ASK YOU TO.
# DONT USE ICONS O EMOJIS IN THE CODE SNIPPETS.
# DONT USES ACENTS IN THE WORDS.

## Project Overview

Alfred is a **local-first AI assistant** with dual frontend architectures:
- **Backend (`Alfred/`)**: FastAPI REST server with RAG (Retrieval-Augmented Generation) using ChromaDB + Ollama LLMs
- **Frontend (`Alfred/`)**: Electron desktop app that auto-manages the backend lifecycle

**Core Philosophy**: 100% local execution for privacy - all documents, embeddings, and LLM inference run on-device with optional GPU acceleration.

## Architecture

### Backend Service (`Alfred/`)
```
alfred_backend.py (FastAPI)
    ↓
alfred_core.py (RAG Logic)
    ↓
┌──────────────┬─────────────────────┐
ChromaDB       Ollama (gemma2:9b)    gpu_manager.py
(embeddings)   (LLM generation)      (CUDA/ROCm/MPS)
```

**Key Files**:
- `alfred_backend.py`: FastAPI endpoints (`/query`, `/history`, `/stats`, `/health`)
- `alfred_core.py`: `AlfredCore` class handles document loading, vector search, and LLM chains
- `config.py`: Prompt templates (Spanish responses, explicit consent for personal data extraction)
- `functionsToHistory.py`: Q&A history with keyword-based similarity scoring using Spanish stopwords
- `db_manager.py`: SQLite database manager for persistent storage (conversations, Q&A history, encrypted data)
- `gpu_manager.py`: Auto-detects NVIDIA/AMD/Apple Silicon GPUs and configures Ollama accordingly

### Frontend (`Alfred/`)
Electron app with **automatic backend management**:
- `main.js`: Spawns `python alfred_backend.py` as child process on startup
- `js/rendering/renderer.js`: IPC communication, chat UI, and typewriter effects
- PowerShell scripts (`start.ps1`, `restart-alfred.ps1`) for development workflow

## Critical Workflows

### Starting the Project
```powershell
# Backend only (development)
cd Alfred
Copy-Item .env.example .env  # Configure ALFRED_DOCS_PATH first
python alfred_backend.py

# Full desktop app (recommended)
cd Alfred
.\start.ps1  # Auto-installs deps, starts backend, launches Electron
```

**Environment Setup**:
- Copy `Alfred/.env.example` → `.env`
- **MUST SET**: `ALFRED_DOCS_PATH` (documents directory for RAG)
- Requires Ollama with models: `gemma2:9b`, `nomic-embed-text:v1.5`

### Testing
```powershell
# Backend API tests
python Alfred/test_backend.py

# GPU detection
python Alfred/test_gpu.py

# Check Ollama GPU usage
python Alfred/test_ollama_gpu.py
```

### Common Issues
1. **Windows Path Length Error**: Documented in `TROUBLESHOOTING_WINDOWS_PATH.md` - use `requirements_core.txt` instead
2. **Backend Not Initializing**: Check Ollama is running (`ollama serve`) and models are pulled
3. **Electron Can't Start Backend**: Verify Python in PATH and `Alfred/alfred_backend.py` exists

## Project-Specific Patterns

### 1. Two-Step Query Optimization
`alfred_core.py` implements smart caching:
1. Search Q&A history (`functionsToHistory.py`) with >60% similarity → instant response
2. If no match → full RAG pipeline (ChromaDB retrieval + LLM generation)

**Code Pattern**:
```python
# In alfred_backend.py query endpoint
if request.use_history:
    history_results = search_in_qa_history(question)
    if history_results and history_results[0]['score'] > 0.6:
        return QueryResponse(answer=..., from_history=True)
# Fall through to full retrieval chain
```

### 2. GPU Manager Singleton
`gpu_manager.py` uses global singleton pattern:
```python
from gpu_manager import get_gpu_manager
gpu_manager = get_gpu_manager()  # Reuses same instance
gpu_manager.configure_ollama_for_gpu()  # Sets env vars for Ollama
```

Always call `configure_ollama_for_gpu()` before initializing `OllamaLLM` to enable hardware acceleration.

### 3. Personal Data Extraction
`alfred_core.py` includes regex patterns for Mexican documents (RFC, CURP, NSS):
```python
def _extract_personal_data(self, text: str) -> Dict[str, str]:
    # Regex patterns in config.py
    return {"rfc": ..., "curp": ..., "nss": ...}
```
This is **explicit by design** - prompts grant full consent for local data extraction.

### 4. Markdown Rendering with Table Support
`renderer/dom/dom-utils.js` includes full Markdown parser with table support:
```javascript
export function markdownToHtml(text) {
    // Supports: code blocks, tables, bold, italic, links, lists, headings
    // Tables processed BEFORE inline code to handle pipe characters
}
```
Tables are rendered with professional styling (gradient headers, hover effects, auto-aligned numbers).

**Table Syntax**:
```markdown
| Column 1 | Column 2 | Column 3 |
|----------|----------|----------|
| Data 1   | Data 2   | Data 3   |
```

See `TABLA_MARKDOWN_EJEMPLOS.md` for full examples and usage patterns.

### 5. Electron-Backend Lifecycle
`Alfred/main.js` manages backend as child process:
```javascript
backendProcess = spawn('python', ['alfred_backend.py'], {cwd: BACKEND_CONFIG.path});
// On app quit
app.on('before-quit', () => stopBackend());
```

**Never start backend manually** when using Electron - it auto-spawns and terminates.

### 6. Async FastAPI with LangChain Sync Chains
`alfred_backend.py` uses `async def` but calls sync LangChain methods:
```python
@app.post("/query")
async def query_alfred(request: QueryRequest):
    result = alfred_core.retrieval_chain.invoke(...)  # Sync call
```
This is intentional - FastAPI handles threading. Don't await LangChain chains.

## Configuration Files

### `.env` (Alfred Backend)
```env
ALFRED_DOCS_PATH=          # REQUIRED: Path to personal documents
ALFRED_MODEL=gemma2:9b     # LLM model name
ALFRED_EMBEDDING_MODEL=nomic-embed-text:v1.5
ALFRED_HOST=127.0.0.1        # API host
ALFRED_PORT=8000           # API port
ALFRED_FORCE_RELOAD=false  # Force document reprocessing
```

### `package.json` (Electron)
- `npm start`: Launch app (spawns backend automatically)
- `npm run dev`: Launch with DevTools
- No build commands needed for development

## API Conventions

### Request/Response Models (Pydantic)
All endpoints use typed Pydantic models in `alfred_backend.py`:
- `QueryRequest`: `{question, use_history, save_response, search_documents}`
- `QueryResponse`: `{answer, personal_data, sources, from_history, timestamp}`

### Error Handling
Backend returns structured JSON errors:
```python
raise HTTPException(status_code=500, detail={"error": "...", "type": "..."})
```

Frontend shows visual notifications via `window.alfredAPI.onBackendNotification()`.

## File Naming Conventions
- Python: `snake_case.py` (e.g., `alfred_backend.py`, `gpu_manager.py`)
- JavaScript: `camelCase.js` (e.g., `renderer.js`) 
- PowerShell: `kebab-case.ps1` (e.g., `start-alfred-server.ps1`)
- Markdown docs: `SCREAMING_SNAKE_CASE.md` for guides (e.g., `QUICKSTART.md`)

## Integration Points

### ChromaDB Persistence
- Directory: `Alfred/chroma_db/` (SQLite + HNSW indexes)
- Reset: Delete directory and restart backend with `ALFRED_FORCE_RELOAD=true`

### Q&A History (SQLite)
- Database: `%AppData%/Alfred/db/alfred.db` (tabla `qa_history`)
- Cifrado: AES-256-GCM para todos los campos (pregunta, respuesta, datos personales)
- Searchable via `/history/search` endpoint con similitud de keywords
- Migracion legacy: `python backend/migrate_json_to_sqlite.py` para convertir JSON antiguo

### Ollama Communication
- HTTP API: `http://localhost:11434` (default Ollama port)
- Models must be pre-pulled: `ollama pull gemma2:9b`
- GPU usage: Automatic via env vars set by `gpu_manager.py`

## Documentation Index
- **Quick Start**: `Alfred/QUICKSTART.md` - 5-minute setup guide
- **API Reference**: `Alfred/README_BACKEND.md` - Full endpoint documentation
- **GPU Setup**: `Alfred/GPU_SETUP.md` - NVIDIA/AMD/Apple Silicon configuration
- **Deployment**: `Alfred/DEPLOYMENT.md` - Production setup
- **Electron Guide**: `Alfred/README.md` - Frontend features
- **Project Structure**: `Alfred/PROJECT_STRUCTURE.md` - Component overview

## Development Notes

1. **Spanish Language**: All user-facing responses are in Spanish (configured in `config.py` prompts)
2. **Windows-First**: Project assumes Windows (`pwsh.exe`, `\` paths) but includes cross-platform fallbacks
3. **SQLite Database**: All persistent data (conversations, Q&A history, metadata) stored in `alfred.db` with AES-256-GCM encryption
4. **LangChain Version**: Uses community integrations (`langchain_community`, `langchain_ollama`)
5. **CORS Enabled**: `allow_origins=["*"]` in FastAPI for C# client compatibility

## When Making Changes

- **Backend API**: Update Pydantic models in `alfred_backend.py` and sync with C# client (`AlfredClient.cs`)
- **Prompts**: Modify `config.py` templates, not hardcoded strings in `alfred_core.py`
- **Electron IPC**: Changes to backend endpoints require updating `renderer.js` fetch calls
- **GPU Detection**: Test on CPU fallback - not all users have dedicated GPUs
- **History Search**: Adjust `functionsToHistory.py` stopwords for non-Spanish deployments
